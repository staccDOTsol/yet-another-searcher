{
  "fileName": "job.py",
  "filePath": "observability/etl/dataflow-etls/job.py",
  "url": "https://github.com/mrgnlabs/marginfi-v2/observability/etl/dataflow-etls/job.py",
  "summary": "The `marginfi-v2` project contains a file with the code above. The code is responsible for extracting events from Solana transactions and writing them to BigQuery tables. The code is written in Python and uses the Apache Beam framework to process data in parallel.\n\nThe `run` function is the entry point of the code. It takes several arguments, including the input table, output table namespace, Solana cluster, minimum IDL version, start date, end date, and beam arguments. The function reads raw transactions from a BigQuery table, extracts events from them, and writes the events to BigQuery tables.\n\nThe `extract_events_from_tx` function takes a transaction as input and returns a list of records. It first decodes the transaction message and metadata and reconciles the instruction logs. It then extracts events from the instructions using the IDL schema and returns a list of records.\n\nThe `create_records_from_ix` function takes an instruction with logs and a versioned program as input and returns a list of records. It first parses the instruction data using the program's coder. It then iterates over the logs and decodes the event data using base64. It finally parses the event data using the program's event coder and creates a record for each event.\n\nThe `extract_events_from_ix` function takes an instruction with logs and a versioned program as input and returns a list of records. It first checks if the instruction program ID matches the program ID of the versioned program. If it does, it calls the `create_records_from_ix` function to extract events from the instruction. It then recursively calls itself on the inner instructions of the instruction.\n\nThe `DispatchEventsDoFn` class is a Beam DoFn that takes a record as input and outputs it to a tagged output based on the record type. The tagged outputs are used to write the records to different BigQuery tables.\n\nThe `run` function defines a Beam pipeline that reads raw transactions from a BigQuery table, extracts events from them, and writes the events to BigQuery tables. It first reads the raw transactions using the `ReadFromBigQuery` transform. It then extracts events from the transactions using the `FlatMap` transform and the `extract_events_from_tx` function. It then dispatches the events to tagged outputs using the `ParDo` transform and the `DispatchEventsDoFn` class. Finally, it writes the records to BigQuery tables using the `WriteToBigQuery` transform.\n\nThe code is designed to be used as part of a larger data processing pipeline that extracts, transforms, and loads data from Solana transactions to BigQuery tables. The code can be customized by changing the input and output tables, Solana cluster, minimum IDL version, start date, end date, and beam arguments.",
  "questions": "1. What is the purpose of this code?\n- This code is a pipeline that extracts events from a BigQuery table and dispatches them to different output tables based on their event type.\n\n2. What external libraries does this code use?\n- This code uses several external libraries including argparse, base64, json, logging, typing, apache_beam, and anchorpy.\n\n3. What is the input and output format of this code?\n- The input format of this code is a BigQuery table, and the output format is a set of BigQuery tables where events are dispatched based on their type."
}